{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,393\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file_path = Path('../data/tinyshakespeare.txt')\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "print(f\"length of dataset in characters: {len(text):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get all the unique characters that occur in this text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a mapping from characters to integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode(\"khoa\") = [49, 46, 53, 39]\n",
      "decode(encode(\"khoa\")) = 'khoa'\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(f\"{encode(\"khoa\") = }\")\n",
    "print(f\"{decode(encode(\"khoa\")) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) <built-in method type of Tensor object at 0x7c74a8195900>\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.type)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have a very small code book of 65 characters, very simple `encode` and `decode` functions, but we get very long sequences as a result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create the train and validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape = torch.Size([1003853])\n",
      "val_data.shape = torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{train_data.shape = }\")\n",
    "print(f\"{val_data.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), target = 47\n",
      "when input is tensor([18, 47]), target = 56\n",
      "when input is tensor([18, 47, 56]), target = 57\n",
      "when input is tensor([18, 47, 56, 57]), target = 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), target = 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), target = 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), target = 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), target = 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context}, target = {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Training Notes**:  \n",
    ">\n",
    "> We will train on all 8 input examples with the context of 1 character upto 8 characters. This is not just because of computational reasons, but also to make our Transformer get used to see inputs of different sizes (upto block size characters).  \n",
    ">\n",
    "> We will stack many batches of multiple chunks of text in a single torch `Tensor`, so we can keep the GPU busy since it is very good at parallel processing of data. These chunks will be processed independently in a parallel manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- getting the first batch of data ----\n",
      "inputs:\n",
      "xb.shape = torch.Size([4, 8])\n",
      "xb = tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]])\n",
      "\n",
      "targets:\n",
      "yb.shape = torch.Size([4, 8])\n",
      "yb = tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
      "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
      "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
      "        [39,  1, 46, 53, 59, 57, 43,  0]])\n",
      "\n",
      "---- there are 32 training examples here ----\n",
      "when input is [53] the target: 59\n",
      "when input is [53, 59] the target: 6\n",
      "when input is [53, 59, 6] the target: 1\n",
      "when input is [53, 59, 6, 1] the target: 58\n",
      "when input is [53, 59, 6, 1, 58] the target: 56\n",
      "when input is [53, 59, 6, 1, 58, 56] the target: 47\n",
      "when input is [53, 59, 6, 1, 58, 56, 47] the target: 40\n",
      "when input is [53, 59, 6, 1, 58, 56, 47, 40] the target: 59\n",
      "when input is [49] the target: 43\n",
      "when input is [49, 43] the target: 43\n",
      "when input is [49, 43, 43] the target: 54\n",
      "when input is [49, 43, 43, 54] the target: 1\n",
      "when input is [49, 43, 43, 54, 1] the target: 47\n",
      "when input is [49, 43, 43, 54, 1, 47] the target: 58\n",
      "when input is [49, 43, 43, 54, 1, 47, 58] the target: 1\n",
      "when input is [49, 43, 43, 54, 1, 47, 58, 1] the target: 58\n",
      "when input is [13] the target: 52\n",
      "when input is [13, 52] the target: 45\n",
      "when input is [13, 52, 45] the target: 43\n",
      "when input is [13, 52, 45, 43] the target: 50\n",
      "when input is [13, 52, 45, 43, 50] the target: 53\n",
      "when input is [13, 52, 45, 43, 50, 53] the target: 8\n",
      "when input is [13, 52, 45, 43, 50, 53, 8] the target: 0\n",
      "when input is [13, 52, 45, 43, 50, 53, 8, 0] the target: 26\n",
      "when input is [1] the target: 39\n",
      "when input is [1, 39] the target: 1\n",
      "when input is [1, 39, 1] the target: 46\n",
      "when input is [1, 39, 1, 46] the target: 53\n",
      "when input is [1, 39, 1, 46, 53] the target: 59\n",
      "when input is [1, 39, 1, 46, 53, 59] the target: 57\n",
      "when input is [1, 39, 1, 46, 53, 59, 57] the target: 43\n",
      "when input is [1, 39, 1, 46, 53, 59, 57, 43] the target: 0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # randomizing the training data \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "print('---- getting the first batch of data ----')\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print('inputs:')\n",
    "print(f\"{xb.shape = }\")\n",
    "print(f\"{xb = }\")\n",
    "print()\n",
    "\n",
    "print('targets:')\n",
    "print(f\"{yb.shape = }\")\n",
    "print(f\"{yb = }\")\n",
    "print()\n",
    "\n",
    "print(f'---- there are {batch_size*block_size} training examples here ----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3724, -0.2800, -0.0915,  ...,  0.8662, -0.9581, -0.9197],\n",
       "         [-1.2796,  0.3641, -0.8859,  ...,  0.5602,  0.6467,  0.6577],\n",
       "         [ 0.4138, -1.4386,  1.2962,  ...,  1.6742, -0.2397,  0.3415],\n",
       "         ...,\n",
       "         [-1.2796,  0.3641, -0.8859,  ...,  0.5602,  0.6467,  0.6577],\n",
       "         [ 0.7763, -0.8460,  0.8437,  ..., -1.0367, -1.2909,  1.1822],\n",
       "         [ 0.3418, -0.9276,  1.2381,  ...,  1.5018, -0.5266,  0.2354]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor(4.9456, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None) -> tuple[torch.Tensor, torch.Tensor | None]:\n",
    "        logits = self.token_embedding_table(idx)  # (batch, time, channel)\n",
    "        # F.cross_entropy expects logits.shape to be [batch_size, num_classes] \n",
    "        # and targets to be of shape [batch_size],\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # e.g. [32, 65]\n",
    "            targets = targets.view(B*T)  # e.g. [32]\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the loss and predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step in the logits\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nl-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\\nvLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "decode(m.generate(idx, max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dk/miniconda3/envs/rupygpt/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.744039535522461\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(1000): # increase number of steps for good results...\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)  # set gradients to 0\n",
    "    loss.backward()  # getting new gradients for all the weights\n",
    "    optimizer.step()  # use the gradients to update the weights\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nN3vVoesMyas:Iocindad.e-NNSqYPso&bFho&$;BQ$dZTMf'mKlf;DRPm'W,esPzyXAzCA$;GunqCEy&Oy;ZxjKVhmrdhxCAbTSp\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "decode(m.generate(idx, max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the \"Self-Attention\"\n",
    "\n",
    "First, let's prepare the inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # batch size, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention V1: averaging past context with `for` loops, the weakest form of aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        # the previous tokens are from 0 up to this current token including it \n",
    "        xprev = x[b, :t+1]  # shape: (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.3596, -0.9152],\n",
       "         [ 0.6258,  0.0255],\n",
       "         [ 0.9545,  0.0643],\n",
       "         [ 0.3612,  1.1679],\n",
       "         [-1.3499, -0.5102],\n",
       "         [ 0.2360, -0.2398],\n",
       "         [-0.9211,  1.5433]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.2858,  0.9651],\n",
       "         [-2.0371,  0.4931],\n",
       "         [ 1.4870,  0.5910],\n",
       "         [ 0.1260, -1.5627],\n",
       "         [-1.1601, -0.3348],\n",
       "         [ 0.4478, -0.8016],\n",
       "         [ 1.5236,  2.5086]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 1.0101,  0.1215],\n",
       "         [ 0.1584,  1.1340],\n",
       "         [-1.1539, -0.2984],\n",
       "         [-0.5075, -0.9239],\n",
       "         [ 0.5467, -1.4948],\n",
       "         [-1.2057,  0.5718],\n",
       "         [-0.5974, -0.6937]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.3514, -0.2759],\n",
       "         [-1.5108,  2.1048],\n",
       "         [ 2.7630, -1.7465],\n",
       "         [ 1.4516, -1.5103],\n",
       "         [ 0.8212, -0.2115],\n",
       "         [ 0.7789,  1.5333],\n",
       "         [ 1.6097, -0.4032]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is ok but very inefficient. Let's do the same thing but with matrix multiplication for better performance*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention V2: Using matrix multiply as weighted aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)  # do this to get the average of the columns in b \n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei = tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)  # wei is `a` above\n",
    "print(f\"{wei = }\")\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3947e-07)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(xbow - xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention V3: Using softmax - More Intuitive \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # masking future tokens' afinities to be 0s\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `wei` matrix tells us that how much each token from the past do we aggregate and average out. For now, we tell that the future can't communicate with the past by making those elements (called \"affinities\") to be `-inf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we normalize `wei` to do weighted aggregations of the past elements (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attetion V4: Self-attention\n",
    "\n",
    "Before, `wei` helps us get the weighted averages of the past tokens. However, we do not want this uniform attentions among all past tokens as <span style=\"color:skyblue\">*some tokens are more interesting than the others, and this should be data dependent. This is the problem that self-attention solves*</span>.\n",
    "\n",
    "The way self-attention solves this is as following:\n",
    "- Every single node / token at each position will emit 2 vectors: *a query* and *a key*. The query vector contains information about what that token is looking for, and the key vector contains information about the infomation that token contains\n",
    "- The way we get affinities between these tokens the dot product between query and key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a single attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k.shape = torch.Size([4, 8, 16])\n",
      "q.shape = torch.Size([4, 8, 16])\n",
      "v.shape = torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)  # learnable from the input tokens\n",
    "k = key(x)  # (B, T, head_size)\n",
    "print(f\"{k.shape = }\")\n",
    "q = query(x)  # (B, T, head_size)\n",
    "print(f\"{q.shape = }\")\n",
    "v = value(x)\n",
    "print(f\"{v.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we forward the `nn.Linear` on `x`, all the tokens in all positions of `(B, T, C)` independently produce a `key` and a `query` -> no communications happen yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply the attention head and masking to get the `wei`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k.transpose(-2, -1).shape = torch.Size([4, 16, 8])\n",
      "wei.shape = torch.Size([4, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{k.transpose(-2, -1).shape = }\")\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, head_size) @ (B, head_size, T) = (B, T, T)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # masking future tokens' afinities to be 0s\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(f\"{wei.shape = }\")\n",
    "\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now, every batch element contains different batch of weights (no more uniform). Now we can get the output of the attention head by multiplying `wei` and `value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention process: \n",
    "\n",
    "<img src=\"../assets/self-attention.gif\" width=\"800\" />\n",
    "\n",
    "\n",
    "End result:\n",
    "\n",
    "<img src=\"../assets/self-attention-static.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Notes**:\n",
    "> - <span style=\"color:skyblue\">***Attention is a communication mechanism***</span>. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "> - <span style=\"color:skyblue\">***There is no notion of space (unlike CNNs)***</span>. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "> - Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "> - In an <span style=\"color:skyblue\">***\"encoder\" attention block, all tokens can communicate (just delete the single line that does masking with `tril`)***</span>. This block here is called a <span style=\"color:skyblue\">***\"decoder\" attention block because it has triangular (`tril`) masking***</span>, and is usually used in autoregressive settings, like language modeling.\n",
    "> - <span style=\"color:skyblue\">***\"self-attention\"***</span> just means that the keys and values are produced from the same source as queries. In <span style=\"color:skyblue\">***\"cross-attention\"***</span>, the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "> - <span style=\"color:skyblue\">***\"Scaled\" attention additional divides `wei` by $1/ \\sqrt{(head\\_size)}$***</span>. This makes it so when input Q, K are unit variance, `wei` will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{head\\_size}}) V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention V5: Scaled Dot-Product Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-scaled attention:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k.var() = tensor(1.0204)\n",
      "q.var() = tensor(1.0632)\n",
      "wei.var() = tensor(14.6308)\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1)  # naive attention\n",
    "\n",
    "print(f\"{k.var() = }\")\n",
    "print(f\"{q.var() = }\")\n",
    "print(f\"{wei.var() = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `wei`'s variance is on the order of the head size (`17.4`)\n",
    "\n",
    "However, with **Scaled Attention**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k.var() = tensor(0.9891)\n",
      "q.var() = tensor(1.0652)\n",
      "wei.var() = tensor(1.0078)\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5  # scaled attention\n",
    "\n",
    "print(f\"{k.var() = }\")\n",
    "print(f\"{q.var() = }\")\n",
    "print(f\"{wei.var() = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:skyblue\">*we can see that the scaled `wei` has variance of almost 1. This is important since `wei` will be fetched into Softmax, which requires it to be quite diffused, otherwise the output will be converged to one-hot vectors and becomes very peaky and the attention will look at the that peaky node*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor=tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7c73ef167c50>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAADFCAYAAABJnHlwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAsHElEQVR4nO3de1xUdfrA8c/McBMFBLkrCt7zAiiKadpN8lJZtllqma79ti3S0qyt9bdb2La/vGSbeUnNanMzb7VZ2SppJJKFYiDeMBXFROQiIMNNbjPn98coyarIwMCZYZ736zWvlxzPnPPMvGYeHr7ne56vRlEUBSGEEDZLq3YAQgghmkYSuRBC2DhJ5EIIYeMkkQshhI2TRC6EEDZOErkQQtg4SeRCCGHjHNQOwBKMRiPnz5/Hzc0NjUajdjhCCNFkiqJQUlJCYGAgWm39NXerSOTnz58nKChI7TCEEMLiMjMz6dSpU737tIpE7ubmBphesLu7u8rRCCFE0xUXFxMUFFSb3+rTKhL5leEUd3d3SeRCCKthMCokZRSSV1KBr5sLkSFe6LTmDf82ZLi4VSRyIYSwNrFHsnl9axrZ+orabQEeLsSM68OYfgEWPZfMWhFCCAuLPZJN9LqUOkkcIEdfQfS6FGKPZFv0fJLIhRDCggxGhde3pnG9trJXtr2+NQ2D0XKNZyWRCyGEBSVlFF5TiV9NAbL1FSRlFFrsnJLIhRDCgvJKbpzEG7NfQ0giF0IIC/J1c7Hofg0hiVwIISwoMsSLAA8XbjRpUINp9kpkiJfFzimJXAghLEin1RAzrs91L3ZeSe4x4/qYPZ+8PpLIhRDCwu7q7Ut7V8drtvt7uLByykCLzyOXG4KEEMLCNu3PpKi8Gj83ZxY9EkpReXWj7+xsCEnkQghhQRXVBlbsSgdgxt3duaOnb7OfU4ZWhBDCgjYmnSW3uJIADxcmDm6ZrqySyIUQwkIqqg28F38KgGfv6o6zg65FziuJXAghLGRD0lnySioJ9HDh0UH19xC3JEnkQghhAVdX4zPubrlqHCSRCyGERXy67ywXSirp2L4Nj0S07IplksiFEKKJLlUZWHmlGr+rO04OLZtaJZELIUQTfbrvV/JLTdX4hIiWGxu/QhK5EEI0waUqA6t2nwbgubtbvhoHSeRCCNEk6/aaqvEgrzY8rEI1DpLIhRCi0cqrali12zQ2/txdPXDUqZNSJZELIUQjfZL4KwVlVXT2cuWhgR1Vi0MSuRBCNEJZZQ2rE0xj4zPv7q5aNQ6SyIUQolE+2fsrhWVVdOngyu8GqFeNgyRyIYQwW1llDe8nXJmp0gMHFatxkEQuhBBmW5t4hsKyKkK82zI+PFDtcCSRCyGEOUrrVOPdVa/GQRK5EEKYZe1PZygqrybEuy0PhKlfjYMkciGEaLCSiuraavz5kdZRjYMkciGEaLC1P51Bf6marj5teSBM3ZkqV5NELoQQDVBcUc2aHzIAmDWyR7MsotxYksiFEKIBPv7RVI13923H/aHWMTZ+hSRyIYS4Cf2laj744crYuHVV4yCJXAghbuqfP2ZQXFFDd9923Nc/QO1wriGJXAgh6qG/VM2He6xzbPwKSeRCCFGPj/ZkUFJRQ08/66zGoZGJfMWKFQQHB+Pi4sKQIUNISkq64b5r1qxhxIgReHp64unpSVRU1DX7//73v0ej0dR5jBkzpjGhCSGExejLq/mothrvidYKq3FoRCLftGkTc+bMISYmhpSUFMLCwhg9ejR5eXnX3T8+Pp7Jkyeza9cuEhMTCQoKYtSoUWRlZdXZb8yYMWRnZ9c+NmzY0LhXJIQQFvLhntOUVNbQ29+Nsf381Q7nhjSKoijmPGHIkCEMHjyY5cuXA2A0GgkKCuK5557jz3/+802fbzAY8PT0ZPny5UydOhUwVeRFRUV8+eWX5r8CoLi4GA8PD/R6Pe7u7o06hhBCXK2ovIrhC3dRWlnDyscHMraFh1XMyWtmVeRVVVUkJycTFRX12wG0WqKiokhMTGzQMcrLy6mursbLy6vO9vj4eHx9fenVqxfR0dEUFBTc8BiVlZUUFxfXeQghhCV98EMGpZer8dF9rbcaBzMTeX5+PgaDAT8/vzrb/fz8yMnJadAxXnnlFQIDA+v8MhgzZgz/+te/iIuLY+HChezevZuxY8diMBiue4z58+fj4eFR+wgKCjLnZQghRL0ullXx8U9nAJgd1cNqx8avcGjJky1YsICNGzcSHx+Pi4tL7fZJkybV/rt///6EhobSrVs34uPjGTly5DXHmTt3LnPmzKn9ubi4WJK5EMJiPthzmtLKGm4JcGdUH+uuxsHMitzb2xudTkdubm6d7bm5ufj71/9iFy9ezIIFC9ixYwehoaH17tu1a1e8vb1JT0+/7v87Ozvj7u5e5yGEEJZQWFbFxz+eAWyjGgczE7mTkxMRERHExcXVbjMajcTFxTF06NAbPm/RokW88cYbxMbGMmjQoJue59y5cxQUFBAQYJ1zNoUQrdeaH05TVmWgb6A7o/r43fwJVsDs6Ydz5sxhzZo1rF27lmPHjhEdHU1ZWRnTp08HYOrUqcydO7d2/4ULF/Lqq6/y0UcfERwcTE5ODjk5OZSWlgJQWlrKn/70J/bu3cuZM2eIi4vjwQcfpHv37owePdpCL1MIIW6uoLSStbVj4z3RaKy/GodGjJFPnDiRCxcu8Nprr5GTk0N4eDixsbG1F0DPnj2LVvvb74eVK1dSVVXFhAkT6hwnJiaGefPmodPpOHToEGvXrqWoqIjAwEBGjRrFG2+8gbOzcxNfnhBCNNz7P5ymvMpAv47uRN3iq3Y4DWb2PHJrJPPIhRBNVVBayfCFu7hUbeCDqYOIUnlYpdnmkQshRGv1fsJpLlUbCO3kwUgbqsZBErkQQpBfWsm/En8FTDNVbGVs/ApJ5EIIu7d69ykuVRsIC2rPXb1sqxqHFr4hSAhbZjAqJGUUkldSga+bC5EhXlbZm1qYJ6+kgk/22m41DpLIhWiQ2CPZvL41jWx9Re22AA8XYsb1YUw/ud/Blq3efZqKaiPhQe25s6eP2uE0igytCHETsUeyiV6XUieJA+ToK4hel0LskWyVIhNNlVdSwTobr8ZBErkQ9TIYFV7fmsb15uhe2fb61jQMRpufxWuXVsWfprLGyIDO7bnDRqtxkEQuRL2SMgqvqcSvpgDZ+gqSMgpbLihhEXnFFXy6z1SNv2BDd3FejyRyIeqRV3LjJN6Y/YT1eC/+FJU1RiK6eDKih7fa4TSJJHIh6uHr5nLznczYT1iHHH0F65POArZfjYMkciHqFRnihU+7+nv++Lk7ExniVe8+wrqs2n2Kqhojg4M9ua17B7XDaTJJ5ELUo6rGiJND/dWaq5NOLnbakKurcVvqcFgfSeRC3ICiKMz94hBZRRW4uTjg61a3Mvd1c8bFUUtGfjlvfJOmUpTCXO/Fp1NVYyQy2Ith3Wy/Gge5IUiIG1r70xm+TD2PTqthzdRBDA72uubOzt0n8njy45/5ZO+vhAe15+GITmqHLepxvugSG5MyAZh9j+3OG/9vUpELcR37zxTy9/8cA2Du2N7c2rUDOq2God068GB4R4Z2M/18d28/Zo3sAcD/bjnMkSy9mmGLm3gvPp0qg5EhIV4M62bbM1WuJolciP+SV1zBs5+mUGNUuD80gP8ZHlLv/rNG9uCuXj5U1hh5Zl0yF8uqWihSYY6sokts2m+qxl+4p6fK0ViWJHIhrlJVY+TZT1O4UFJJLz83Fk0Ivemf31qthiUTB9DZy5VzFy8xa1OqXPy0Qu/tSqfaoDC0awdu7do6xsavkEQuxFXe3HaMn3+9iJuzA6ueiMDVqWGXkTxcHVn9RAQujloSTlxgyXcnmjlSYY5zF8vZ/PPlsfGoHipHY3mSyIW4bMuBc3x8eeHddyaGE+Ld1qzn3xLgzoLfhQKw7Pt0dhzNsXSIopFW7DpFtUFhWLcODGll1ThIIhcCgLTzxcz94jAAz9/dvdHrNY4f0JHfDwsG4MXNBzl9odRSIYpGyiws57OfW+fY+BWSyIXdKyqv4ul1P1NRbeSOnj7Mimral/0v993C4GBPSipreGZdMmWVNRaKVDTGil3p1BgVhnf3ZnBw67wDVxK5sGtGo8LsTalkFl4iyKsN704Kb/KqP446LSseG4iPmzMnckt5+d+HUBS5+KmGzMJyPk8+B8AL97S+sfErJJELu7Yk7iTxxy/g7KBl1ZQI2rs6WeS4vu4urHx8IA5aDf85lM2HezIsclxhnuXfm6rxET28iejSOqtxkEQu7FjcsVyWxp0EYP7v+tM30MOixx8U7MWr9/cxHX/7LySeKrDo8UX9zhaU83mKqRqf3cThMmsniVzYpTP5ZczelArA1KFd+N3A5rm1furQLjw0oCMGo8LM9Slk6y81y3nEtZZ9fxKDUeH2nj5EdPFUO5xmJYlc2J3yqhqe/iSZkooaIrp48tf7+jTbuTQaDW8+1J9bAtwpKKsiel0KlTWGZjufMDmTX8YXB7IAeKEVzhv/b5LIhV1RFIU///swx3NL8HFz5r3HB+Lk0LxfgzZOOlZPicCjjSOpmUX8bat0Smxuy75Px2BUuLOXDwM6t+5qHCSRCzvz0Y9n+PrgeRy0GlY8NhA/95ZZ2adzB1eWTApHo4FP952tvctQWN6Z/DK+TDVV4619bPwKSeTCbuw7XcCb20wdDf9y3y0tvqrPXb18eeFyYvnrl0c4fE46JTaHpZfHxu/u7Ut4UHu1w2kRksiFXcjRVzBj/QEMRoUHwwNr775saTPv6k7ULb5USafEZnH6QilfXh4bv9Je2B5IIhetXlWNkehPk8kvraS3vxvzf9dftQUFtFoNbz8aTnAHV7KKLvH8xgPSKdGCln2fjlGBkb19CbOTahwkkQs78MY3aRw4W4S7iwOrzeho2Fw82jiy6okI2jjq+OFkPm/vOK5qPK3FqQulfGVnY+NXSCIXrdrnyef4ZO+vACyZFE6XDuZ1NGwuvf3dWTjB1CnxvfhTfCudEptsadxJjApE3eJH/06WvbnL2kkiF63WkSw9f9li6mg4a2QP7u7duI6GzeWBsECevM20+tCLmw9ySjolNlp6XilfHzwPtM5+4zcjiVy0ShfLqnhmXTKVNUbu6uVjtRe+5t7bm8gQL0orTTcplUqnxEZZGncSRYFRffzo19G+qnGQRC5aIYNRYdamVM5dvERnL1eWTByAtokdDZvLlU6Jfu7OpOeV8vLnB6VToplO5paw9ZCpGp9lh9U4SCIXrdA7O0+QcOICLo5aVj8RgYero9oh1ct0h2kEjjoN2w7nsOaH02qHZFPevVyNj+7rZ/HGZ7ZCErloVXYczWH5rnQAFvwulFsC3FWOqGEiunjy2ri+ACzY/gs/peerHJFtOJFbwn8OZwP2N1Plao1K5CtWrCA4OBgXFxeGDBlCUlLSDfdds2YNI0aMwNPTE09PT6Kioq7ZX1EUXnvtNQICAmjTpg1RUVGcPHmyMaEJO3b6Qikvbj4IwO+HBTN+QEeVIzLPlCGdeXhgJ4wKzNxwgPNF0inxZt79zlSNj+3nbzO/tJuD2Yl806ZNzJkzh5iYGFJSUggLC2P06NHk5eVdd//4+HgmT57Mrl27SExMJCgoiFGjRpGVlVW7z6JFi1i6dCmrVq1i3759tG3bltGjR1NRUdH4VybsStnli4UllTUMDvbkL/fdonZIZtNoNPzfQ/3oG+hOYVkV0euSqaiWTok3cjznt2rcXsfGaylmioyMVGbMmFH7s8FgUAIDA5X58+c36Pk1NTWKm5ubsnbtWkVRFMVoNCr+/v7KW2+9VbtPUVGR4uzsrGzYsKFBx9Tr9Qqg6PV6M16JaC2MRqPy7KfJSpdXvlEG/32nkqu/pHZITXK2oEwJe/1bpcsr3yh//vdBtcOxWtHrfla6vPKN8uy6ZLVDaRbm5DWzKvKqqiqSk5OJioqq3abVaomKiiIxMbFBxygvL6e6uhovL1PDooyMDHJycuoc08PDgyFDhtzwmJWVlRQXF9d5CPv14Z4M/nMoGwethpVTBuLbQh0Nm0uQlytLJw1Ao4ENSZls2n9W7ZCszrHsYrYdzkGjgeetdGppSzIrkefn52MwGPDzq3tjhZ+fHzk5Dbsz7ZVXXiEwMLA2cV95njnHnD9/Ph4eHrWPoKAgc16GaEUSTxUwf/svALx6f59Wsy7j7T19ePEe08W7V786ysHMInUDsjLvfme6hnZv/wB6+bupHI36WnTWyoIFC9i4cSNbtmzBxaXxVdPcuXPR6/W1j8xM6e1sj7L1l5i5PgWDUeGhAR2ZOrSL2iFZ1LN3difqFj9T0691yRSUVqodklU4el5P7FFTNT5bqnHAzETu7e2NTqcjNze3zvbc3Fz8/f3rfe7ixYtZsGABO3bsIDQ0tHb7leeZc0xnZ2fc3d3rPBrDYFRIPFXAV6lZJJ4qkC50NqSyxkD0uhQKyqq4JcCdNx9Sr6Nhc9FqNfxjYhgh3m05r6/g+Y0HqDEY1Q5LdVcWzL4/NJAeflKNg5mJ3MnJiYiICOLi4mq3GY1G4uLiGDp06A2ft2jRIt544w1iY2MZNGhQnf8LCQnB39+/zjGLi4vZt29fvcdsqtgj2Qxf+D2T1+xl1sZUJq/Zy/CF3xN7JLvZziks5/WtaaRmFuHRxpHVUyJo46RTO6Rm4e7ieLljo44f0wtYvOOE2iGp6uh5Pd8ezUWjgVkju6sdjtUwe2hlzpw5rFmzhrVr13Ls2DGio6MpKytj+vTpAEydOpW5c+fW7r9w4UJeffVVPvroI4KDg8nJySEnJ4fSUlODII1Gw+zZs/n73//O119/zeHDh5k6dSqBgYGMHz/eMq/yv8QeySZ6XQrZ+rrTG3P0FUSvS5FkbuU2/5zJ+n1n0WhMHQ07d3BVO6Rm1dPPjUWXOyWu2n2K7Yft9/O55PLY+ANhgXT3lWr8CrMbM0+cOJELFy7w2muvkZOTQ3h4OLGxsbUXK8+ePYtW+9vvh5UrV1JVVcWECRPqHCcmJoZ58+YB8PLLL1NWVsYf//hHioqKGD58OLGxsU0aR78Rg1Hh9a1pXG8QRQE0mKq9e/r4o7PS/hz27PA5PX/98ggAL0T15K5evipH1DLuDw3kYGYRa37I4KXPDtLDr53dJbIjWXp2puWi1cBzd8vY+NU0imL7HXqKi4vx8PBAr9ffdLw88VQBk9fsvekxNzx1K0O7dbBUiMICCsuqGLdsD1lFl4i6xZf3nxhktc2wmkONwciUD/ex93Qh3Xza8uWM23Bzse4+Mpb0h7X7+e5YHuPDA1kyaYDa4TQ7c/Ka3fVayStp2N2iDd1PtAyDUeH5DQfIKrpEcAdX3n403K6SOICDTsvyxwbi7+7CqQtl/OmzQ3bTKfHQuSK+O5aHVuaNX5fdJXJft4YN1zR0P9Ey3t5xnD3p+bRx1LHqiQg82thPJXo173bOrJwyECedltijOazabR+dEq/MGx8f3pGuPu1Ujsb62F0ijwzxIsDDhfpqOZ2Gev9ftKzYIzm8F38KgIUTQuntb7/NkQAGdPYk5oE+ALz17S/sOdm6OyUezCwi7pc8dFoNz0k1fl12l8h1Wg0x40xfghsla4MCkz/Yy5vbjknTIpWl55Xy0memjoZP3hbCA2GBKkdkHR6L7MwjEaZOic9tSOHcxXK1Q2o2S74zTbkcH96REG/rWHPV2thdIgcY0y+AlVMG4u9Rd/gkwMOFdx4NY+KgIBQF3k84zQPL93AkS69SpPattLKGZ9aZlj+LDPFi7r291Q7Jamg0Gt4Y34/+HT24WF5N9LqUVll0HDh7kV3HL5iq8btl3viN2N2slasZjApJGYXklVTg6+ZCZIhX7ZTD79Jy+fMXh8kvrcRBq2HWyB5E39kNB51d/u5rcYqiMGN9CtsO5+Dn7sw3z43Ax81Z7bCszrmL5YxbtoeL5dU8OqgTCx8ObVV3uE77KIndJy4wIaITix8JUzucFiWzVhpIp9UwtFsHHgzvyNBuHerMG4/q48eOF27n3v7+1BgV3t55godXJcpK5y3k/YTTbDucg6NOw3uPR0gSv4FOnq4snTwArQY2/3yODUmtp+9Q8q8X2X1CqvGGsOtEfjNebZ1Y8dhA3p0UjruLAwczi7j33R/4548ZGKUvS7P5KT2fhbGmjoavjetLRBdPlSOybiN6+PDS6F4AzPv6KKmtpFPiu5d7qjw8sCNdOsjYeH0kkd+ERqPhwfCOfPvC7Yzo4U1ljZHXt6Yx5cN9ZMlSXBZ3vugSMzccwKjAwwM7MWVIZ7VDsgnRd3RjdF8/qgymTon5Nt4pMfnXiyScuICDViN3cTaAJPIGCvBow7+ejOSN8f1o46jjp1MFjHkngc+Tz9nNTRnNraLaQPS6ZArLqugb6M7/PdSvVY33NieNRsPiR8Lo6tOWbH0FM9en2HSnxCszVSZEdCLIq3X30rEESeRm0Gg0PHFrF7bPGsHAzu0pqazhpc8O8sdPbL8Csgavbz3KwXN62rs6smpKBC6OrbOjYXNxczF1gmzrpGPv6UIWfXtc7ZAa5eczhfxwMh8HrYYZd8nYeENIIm+EYO+2fPbMMF4e0wtHnYadabmMfieB2CMNWyVJXGtj0lk2JGWi0cDSSQOkCmukHn5uvHV5dsf7Caf5zyHb65T4zuVq/JFBUo03lCTyRtJpNTx7Z3e+mjGc3v5uFJRV8cy6ZOZsTkV/qVrt8GzKwcwiXvvqKAAv3tOT23v6qByRbbu3fwBP394VgD99fpATuSUqR9RwSRmF/JhegKNOqnFzSCJvoj6B7nw18zai7+yGVgNfpGQxZklCq79t2lIKSiuJXpdMlcHIPX38ePZO+fJawp9G92JYtw6UVxl45pNkiitso7hYUluNB9HJU6rxhpJEbgHODjpeGdObz54ZSpcOrmTrK5jy4T7mfX2US1Wt7247S6kxGHl+4wHO6yvo6t2Wtx8Ns7uOhs3FQadl2eQBBHq4cDq/jJc2H7T6KbP7Thfw0ympxhtDErkFRXTxYvusETxxq2kR4I9/OsN9S3/gwNmLKkdmnd7acZwf0wtwdTJ1NHS3o97aLaFDO2dWTonASadlR1ouK3efUjukel0ZG584OIiO7duoHI1tkURuYa5ODrwxvh//ejISf3dTNfTwyp9Y/O1xqmpsdzqYpW0/nM3qyy1YF00IpacsotsswoLa87cH+wKweMdxEk5cUDmi60s8VcDe04U46bQyvNYIksibye09ffh29u2MDw/EqMDyXemMX/Ejx3Ns58JTc0nPK6ntaPjUiBDuD5WOhs1pUmRnJg02NYJ7fuMBMgutq1Oioih1qvFAqcbNJom8GXm4OrJk0gDee3wgnq6OpGUXM27ZHlbvPoXByscrm0tJRTV//CSZsioDt3b14pUx0tGwJcx7oC+hnTwoKq8m+tNkq+qUmHi6gKSMy9X4Xd3UDscmSSJvAff2D+DbF25nZG9fqgxG5m//hUnvJ3K2wLoqo+amKAp/+uwQpy+U4e/uwvLHBko3yRbi4qhj5ZQIvNo6cSSrmL9+ecQq7khWFIUlO009VSZHBhHgIdV4Y8i3qIX4urnwwbRBLHo4lHbODuw/c5Ex7yawft9Zq/hCtYRVu08TezQHJ52WlVMG4t1OOhq2pI7t27D8cqfEz5PP8em+s2qHxE+nCkg6U4iTg5ZnZaZKo0kib0EajYZHBwexfdYIhoR4UV5l4H+3HGb6x/vJLW7diz3vOZnPW9+aOhrGPNCHAZ2lo6EahnX35uXLw1mvbz1KioozqhRF4Z2dprHxxyI74+cu6+Q2liRyFQR5ubLhqVv563234OSgJf74BUa9k8DXB8+rHVqzOHexnOc2pGBU4NFBnXgsUjoaqunp27sytp8/1QaF6HXJXChRp0/QnvR8fv71Is4OWp69U8bGm0ISuUq0Wg1/GNGV/zw3nP4dPdBfqub5DQeYuT6Fi2VVaodnMaaOhilcLK+mf0cP/vagdDRUm0aj4a1Hwuju247c4kpVOiXWqcaHdMZXqvEmkUSush5+bnzx7DBmR/VAp9XwzaFsRi9JYNfxPLVDazJFUXjtqyMcztLj6erIyikDpaOhlWjn7MCqKRG0c3ZgX0YhC7b/0qLn/+FkPilni3B20BJ9h1TjTSWJ3Ao46rTMjurJlmeH0c2nLXkllUz/537mfnGI0soatcNrtA1JmWz++RxaDSydPEB6Z1iZ7r7tatfB/GBPRosN7V09b3zKrV2kGrcASeRWJLRTe/7z/Aj+Z3gIGo0pEY59N4GkjEK1QzPbgbMXmfe1qaPhS6N7MaKHdDS0RmP6+RN9eXz6lc8PtcgNa7tPXODA2SJcHLU8I9W4RUgitzIujjpevb8P6/9wKx3btyGz8BIT30/kzW3HrOomjvrkl1YSvS6FKoOR0X395E9nK/fSqF4M7+7NpWoDz6xLbtY2zKZq3DRv/Ilbu8ii2hYiidxKDe3WgdjZI3h0UCcUxbRIwAPL93AkS692aPWqMRiZuT6FnOIKuvq0ZfEjYXJx08rptBqWTh5Ax/ZtyMgv48XNqc3WKTH++AUOZpqq8aflF7zFSCK3Ym4ujiyaEMYHUwfh3c6ZE7mljF/xI8viTlrteoyLvj3O3tOFtHXS8f4TEbhJR0Ob4NXWiVVTInBy0PLdsTxW7Eq3+DmuHhufOjRYbgizIEnkNiCqjx87Xridsf38qTEqvL3zBA+vSuTUhVK1Q6vjm0PneT/B1NHQNL1NOhrakv6dPPj7g/0A+Md3J4i38MypXcfzOHROTxtHHX+8vIKRsAxJ5DbCq60T7z0+kCUTw3FzceBgZhH3Lf2Bj3/MsIoFA07klvDy54cAePqOrtzbP0DliERjPDo4iMeGdEZRYNbGVIv1A1IUhSWXx8anDusi1biFSSK3IRqNhvEDOrLjhdsZ0cObimoj87amMeXDfWQVXVItruKKap75JJnyKgPDunXgT6N6qRaLaLqYcX0ID2qP/lI1z6xLtsgqV3HHTNW4q5OOp2+XsXFLk0RugwI82vCvJyN5Y3w/2jjq+OlUAWPeSeDz5HMt3oDLaFR4cfNBTueXEejhwrLJA6SjoY1zdtCxcspAOrR1Ii27mL9sOdykz5WiKCyJM42NTxsWjFdbJ0uFKi6Tb5yN0mg0PHFrF7bNGsHAzu0pqazhpc8O8vQnyeSXtlzvjJW7T7EzLfdyR8MIOsifzK1CgEcblj02AJ1WwxcHsvhk76+NPtbOtFyOZBXT1knHUyNkbLw5SCK3cSHebfnsmWG8PKYXjjoNO9JyGf1OArFHcpr93AknLrB4x3EA/vZgX8KC2jf7OUXLGdbNmz9f7pT4t61pJP9q/o1pV4+NSzXefCSRtwI6rYZn7+zOVzOG09vfjYKyKp5Zl8yczakUVzTPzR2ZheU8v/EAigKTBgcxSToatkp/GBHCfaEB1BgVotelkFdiXrvlHWm5pGUX087ZQarxZtSoRL5ixQqCg4NxcXFhyJAhJCUl3XDfo0eP8vDDDxMcHIxGo2HJkiXX7DNv3jw0Gk2dR+/esgSYufoEuvPVzNuIvrMbWg18kZLFmHcS+DE936Lnqbh8B2BReTVhnTyY90Bfix5fWA+NRsOih0Pp4duOvJJKZn56gOoG3sNgNP5Wjf9+WDCeUo03G7MT+aZNm5gzZw4xMTGkpKQQFhbG6NGjycu7/pzT8vJyunbtyoIFC/D397/hcfv27Ut2dnbtY8+ePeaGJjBdqHplTG8+e2YoXTq4cl5fweMf7GPe10ctMvtAURT+suUIR88X49XWiZVTIqSjYSvX1tmB1U9E4ObsQNKZQt7cdqxBz9uRlsOx7GLcnB34w4iQZo7SvpmdyP/xj3/w1FNPMX36dPr06cOqVatwdXXlo48+uu7+gwcP5q233mLSpEk4O9/4QpiDgwP+/v61D29vb3NDE1eJ6OLFtudHMOVW05DHxz+d4b6lP3CgiSvCrNt3ln+nmDoaLp88QFY8txNdfdrx9qOmTon//PEMX6Vm1bv/1dX49NuCae8q1XhzMiuRV1VVkZycTFRU1G8H0GqJiooiMTGxSYGcPHmSwMBAunbtyuOPP87ZszdeT7CyspLi4uI6D3Gtts4O/H18f9Y+GYmfuzOn88t4eOVPvL3jOFU15t/in/zrRf621dTR8JUxvRnWXX7Z2pNRff2ZeXldzVf+fYhj2Tf+3sUezeGXnBLcXBz4n+EyNt7czErk+fn5GAwG/Pz86mz38/MjJ6fxsySGDBnCxx9/TGxsLCtXriQjI4MRI0ZQUnL9lprz58/Hw8Oj9hEUFNToc9uDO3r6sGP2HYwPD8SowLLv03novR/Nall6oaSSZz9NptqgcG9/f7nF2k69cE9Pbu/pQ0W18YadEo1GhXcvV+NP3haCh6v022luVjFrZezYsTzyyCOEhoYyevRotm3bRlFREZs3b77u/nPnzkWv19c+MjMzWzhi2+Ph6siSSQN47/GBeLo6cvR8MeOW7WH17lMYbnKLf7XByIz1KeQWV9Ldtx2LJkhHQ3ul02p4d2I4nTzb8GtBOS9surZT4vYjORzPNVXjTw6XsfGWYFYi9/b2RqfTkZubW2d7bm5uvRcyzdW+fXt69uxJevr1O7A5Ozvj7u5e5yEa5t7+AXz7wu2M7O1LlcHI/O2/MOn9xDo9NQxGhcRTBXyVmkXiqQLe3HaMpIzCOsuDCfvleblTorODlu9/yWPp9ydrPzNfHshi/vY0AP5neAgebaQabwlmfSOdnJyIiIggLi6O8ePHA2A0GomLi2PmzJkWC6q0tJRTp07xxBNPWOyY4je+bi58MG0Qn/18jte3HmX/mYuMeTeBv97XB09XR/72TRrZ+mvnCy++vGCvEP06evB/D/Xnpc8OsuS7k6z96QwXy38bZtEAnb1kab+WYnZpNWfOHKZNm8agQYOIjIxkyZIllJWVMX36dACmTp1Kx44dmT9/PmC6QJqWllb776ysLFJTU2nXrh3du5sunLz00kuMGzeOLl26cP78eWJiYtDpdEyePNlSr1P8F41Gw6ODgxjarQMvfXaQfRmF/O+Wwzd5lvpdFoX1mBDRia8PZpFwIr9OEgfTJ+XFzQdxddIxpp90wmxuZifyiRMncuHCBV577TVycnIIDw8nNja29gLo2bNn0Wp/G7E5f/48AwYMqP158eLFLF68mDvuuIP4+HgAzp07x+TJkykoKMDHx4fhw4ezd+9efHxkncfmFuTlyoanbuWDPad5c9uNV1LXAK9vTeOePv7otDI+LkxDcCdy6u+JL5+ZlqFRWrpdXjMoLi7Gw8MDvV4v4+WNlHiqgMlr9t50vw1P3crQbh1aICJh7eQz07zMyWtWMWtFqK+hPTTM7bUhWi/5zFgPSeQCMF0AteR+ovWTz4z1kEQuAIgM8SLAw4UbjWRqgAAPFyJDvFoyLGHF5DNjPSSRC8B0o0fMuD4A13wxr/wcM66PXLQSteQzYz0kkYtaY/oFsHLKQPw96v4p7O/hwsopA2UambiGfGasg8xaEdcwGBWSMgrJK6nA1830p7FUVaI+8pmxPHPyWqu41/rK7yLpgmg5fX0c6etjur26rLThzbWE/ZLPjGVdyWcNqbVbRSK/0iVRuiAKIVqbkpISPDw86t2nVQytGI1Gzp8/j5ubm9ld+YqLiwkKCiIzM1OGZa4i78v1yftyY/LeXF9j3xdFUSgpKSEwMLDO3fLX0yoqcq1WS6dOnZp0DOmieH3yvlyfvC83Ju/N9TXmfblZJX6FzFoRQggbJ4lcCCFsnN0ncmdnZ2JiYupdGNoeyftyffK+3Ji8N9fXEu9Lq7jYKYQQ9szuK3IhhLB1ksiFEMLGSSIXQggbJ4lcCCFsnCRyIYSwcXadyFesWEFwcDAuLi4MGTKEpKQktUNSXUJCAuPGjSMwMBCNRsOXX36pdkhWYf78+QwePBg3Nzd8fX0ZP348x48fVzssq7By5UpCQ0Nr71wcOnQo27dvVzssq7NgwQI0Gg2zZ8+2+LHtNpFv2rSJOXPmEBMTQ0pKCmFhYYwePZq8vDy1Q1NVWVkZYWFhrFixQu1QrMru3buZMWMGe/fuZefOnVRXVzNq1CjKysrUDk11nTp1YsGCBSQnJ/Pzzz9z99138+CDD3L06FG1Q7Ma+/fvZ/Xq1YSGhjbPCRQ7FRkZqcyYMaP2Z4PBoAQGBirz589XMSrrAihbtmxROwyrlJeXpwDK7t271Q7FKnl6eioffPCB2mFYhZKSEqVHjx7Kzp07lTvuuEOZNWuWxc9hlxV5VVUVycnJREVF1W7TarVERUWRmJioYmTCVuj1egC8vGQ9yqsZDAY2btxIWVkZQ4cOVTscqzBjxgzuu+++OvnG0lpF90Nz5efnYzAY8PPzq7Pdz8+PX375RaWohK0wGo3Mnj2b2267jX79+qkdjlU4fPgwQ4cOpaKignbt2rFlyxb69Omjdliq27hxIykpKezfv79Zz2OXiVyIppgxYwZHjhxhz549aodiNXr16kVqaip6vZ7PP/+cadOmsXv3brtO5pmZmcyaNYudO3fi4uJy8yc0gV0mcm9vb3Q6Hbm5uXW25+bm4u/vr1JUwhbMnDmTb775hoSEhCb3wG9NnJyc6N69OwARERHs37+fd999l9WrV6scmXqSk5PJy8tj4MCBtdsMBgMJCQksX76cyspKdDqdRc5ll2PkTk5OREREEBcXV7vNaDQSFxcn43riuhRFYebMmWzZsoXvv/+ekJAQtUOyakajkcrKSrXDUNXIkSM5fPgwqamptY9Bgwbx+OOPk5qaarEkDnZakQPMmTOHadOmMWjQICIjI1myZAllZWVMnz5d7dBUVVpaSnp6eu3PGRkZpKam4uXlRefOnVWMTF0zZsxg/fr1fPXVV7i5uZGTkwOYVnBp06aNytGpa+7cuYwdO5bOnTtTUlLC+vXriY+P59tvv1U7NFW5ubldcw2lbdu2dOjQwfLXViw+D8aGLFu2TOncubPi5OSkREZGKnv37lU7JNXt2rVLAa55TJs2Te3QVHW99wRQ/vnPf6odmuqefPJJpUuXLoqTk5Pi4+OjjBw5UtmxY4faYVml5pp+KP3IhRDCxtnlGLkQQrQmksiFEMLGSSIXQggbJ4lcCCFsnCRyIYSwcZLIhRDCxkkiF0IIGyeJXAghbJwkciGEsHGSyIUQwsZJIhdCCBv3/4KlNR8bMudhAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 400x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensor = torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n",
    "print(f\"{tensor = }\")\n",
    "# Plot the tensor\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.plot(tensor.numpy(), marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor = tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7c73eefe9a90>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensor = torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1) # gets too peaky, converges to one-hot\n",
    "print(f\"{tensor = }\")\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.plot(tensor.numpy(), marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Transformer\n",
    "\n",
    "We will build the <span style=\"color:skyblue\">***multi-headed self-attention***</span>, which is constructed by simply <span style=\"color:skyblue\">*applying multiple self-attentions in parallel*</span>. We will also learn the <span style=\"color:skyblue\">***feedforward layers of the transformer block***</span> and some optimization tricks to make the network train better (<span style=\"color:skyblue\">***residual connections, layernorm, dropout***</span>) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, n_embd = 384, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd = 384, dropout=0.2):\n",
    "        super().__init__()\n",
    "        # make multiple heads\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # run all heads and concat the results into a list over the channel dimension\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Transformer\n",
    "\n",
    "### Encoder vs. Decoder vs. both (?) Transformer\n",
    "Below we have the architecture of the Transformer from the \"Attention is all you need\" paper\n",
    "<img src=\"../assets/Transformer.png\" width=\"300\" />.\n",
    "\n",
    "We can see that it has an Encoder (Input Embedding) and a Decoder (Output Embedding). <span style=\"color:red\">***In our code, we only have the Decoder***</span> since we are only generating texts from the input data. \n",
    "- The Decoder is made with the triangular mask so it has the auto regressive property where we can just go and sample from it. \n",
    "- In the paper, the setting is translating French to English. Hence the Encoder is used to encode the French input texts\n",
    "- The Encoder does not have the triangular mask since it can see everything from the input texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=9&t=957s)\n",
    "- [Attention is all you need paper](https://arxiv.org/pdf/1706.03762)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rupygpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
